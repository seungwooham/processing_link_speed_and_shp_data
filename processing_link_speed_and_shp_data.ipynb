{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Get a shp file of seoul road network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from os import listdir\n",
    "\n",
    "import itertools\n",
    "from tqdm import tqdm \n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import geopandas"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 1. Load data and select service link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the intersection of service link in speed data is the same number of links (5106)\n",
    "\n",
    "for i, file in enumerate(listdir(os.getcwd()+'/seoul_speed_raw_data')):\n",
    "    if i>=12: # Select data from 2022_1001_1010\n",
    "        speed_data = pd.read_csv('./seoul_speed_raw_data/'+file)\n",
    "        if i==12:\n",
    "            print(f'{file} has initialized the link id list')\n",
    "            link_id_list = speed_data['LINK_ID'].unique()\n",
    "            link_id_list.sort()\n",
    "            print(f'The number of link id in speed data is {len(link_id_list)}')\n",
    "        else:\n",
    "            new_link_id_list = speed_data['LINK_ID'].unique()\n",
    "            new_link_id_list.sort()\n",
    "            if not np.array_equal(link_id_list, new_link_id_list):\n",
    "                print(f'{file} has different link id')\n",
    "                print(len(link_id_list), len(new_link_id_list))\n",
    "                print(np.setdiff1d(new_link_id_list, link_id_list))\n",
    "                # Print which link is different\n",
    "                print(len(np.intersect1d(link_id_list, new_link_id_list)))\n",
    "                link_id_list = np.intersect1d(link_id_list, new_link_id_list)\n",
    "                print('new link list created')\n",
    "                \n",
    "            else:\n",
    "                print(f'{file} has same link id')\n",
    "del file, i, speed_data, link_id_list, new_link_id_list\n",
    "\n",
    "# 2022_1001_1010_speed_5min.txt has intersection of link id in speed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handmade metropolitan suburbs link shp file, September 5th, 2022\n",
    "metropolitan_link = geopandas.read_file('[2022-09-05]NODELINKDATA/metropolitan_link.shp', encoding='utf-8')\n",
    "metropolitan_link = metropolitan_link.apply(pd.to_numeric, errors='ignore')\n",
    "metropolitan_link.sort_values(by=['LINK_ID'], inplace=True)\n",
    "print(f'The length of metropolitan link data = {len(metropolitan_link)}')\n",
    "\n",
    "metropolitan_node = geopandas.read_file('[2022-09-05]NODELINKDATA/metropolitan_node.shp', encoding='utf-8')\n",
    "metropolitan_node = metropolitan_node.apply(pd.to_numeric, errors='ignore')\n",
    "metropolitan_node.sort_values(by=['NODE_ID'], inplace=True)\n",
    "print(f'The length of metropolitan node data = {len(metropolitan_node)} \\n')\n",
    "\n",
    "# Check the intersection of service link data -> Service link of 2021_0901_0910 is the intersection of whole data\n",
    "load_service_link_list = pd.read_csv('seoul_speed_raw_data/2022_1001_1010_speed_5min.txt')\n",
    "service_link_list_from_speed = load_service_link_list['LINK_ID'].unique()\n",
    "del load_service_link_list\n",
    "service_link_list_from_speed.sort()\n",
    "\n",
    "# Utilization of standard link-service link mapping data in Seoul as of April 2022\n",
    "seoul_link_mapping = pd.read_csv('서울시_표준링크_매핑정보_2022년4월_기준.csv')\n",
    "print(f'The length of seoul mapping data = {len(seoul_link_mapping)}')\n",
    "print(f\"Number of uniuqe standard link id == {len(seoul_link_mapping['표준링크아이디'].unique())}\")\n",
    "\n",
    "if (len(seoul_link_mapping['표준링크아이디'].unique()) == len(seoul_link_mapping)):\n",
    "    print(f'Every standard link id is unique \\n')\n",
    "else:\n",
    "    print(f'Every standard link id is not unique \\n')\n",
    "    \n",
    "print(f\"Number of cases where [standard link id==service link id] in seoul mapping data = \",\n",
    "      f\"{sum(seoul_link_mapping['표준링크아이디'] == seoul_link_mapping['서비스링크'])}\")\n",
    "print(f\"Number of cases where [standard link id.isna()] in seoul mapping data = \",\n",
    "      f\"{sum(seoul_link_mapping['표준링크아이디'].isna())}\")\n",
    "print(f\"Number of cases where [service link id.isna()] in seoul mapping data = \",\n",
    "      f\"{sum(seoul_link_mapping['서비스링크'].isna())} \\n\")\n",
    "\n",
    "A = seoul_link_mapping[\n",
    "    ~(seoul_link_mapping['표준링크아이디'] == seoul_link_mapping['서비스링크']) &\n",
    "    ~seoul_link_mapping['서비스링크'].isna()]['서비스링크'].unique()\n",
    "B = seoul_link_mapping[\n",
    "    ~(seoul_link_mapping['표준링크아이디'] == seoul_link_mapping['서비스링크']) &\n",
    "    seoul_link_mapping['서비스링크'].isna()]['표준링크아이디'].unique()\n",
    "\n",
    "print(f'{len(np.intersect1d(A, B))} data in A and B is an intersection. However, ignore this.')\n",
    "del A, B"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Alt text](utils/seoul_mapping_relation.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert standard link id to service link where it is nan value\n",
    "seoul_link_mapping.loc[seoul_link_mapping['서비스링크'].isna(), ['서비스링크']]=seoul_link_mapping[seoul_link_mapping['서비스링크'].isna()]['표준링크아이디']\n",
    "seoul_link_mapping['서비스링크'] = seoul_link_mapping['서비스링크'].astype('int64')\n",
    "\n",
    "# There are service links that exist only in the speed data and not in the mapping data.\n",
    "not_in_mapping_list = list(set(service_link_list_from_speed) - set(seoul_link_mapping['서비스링크'].unique()))\n",
    "print(f'Among {len(service_link_list_from_speed)} service links in speed data, {len(not_in_mapping_list)} are not in mapping data')\n",
    "\n",
    "# Insert those data\n",
    "add_to_list = metropolitan_link[metropolitan_link['LINK_ID'].isin(not_in_mapping_list)]['LINK_ID'].values\n",
    "seoul_link_mapping = pd.concat([seoul_link_mapping, pd.DataFrame(np.vstack((add_to_list, add_to_list)).T, columns=seoul_link_mapping.columns)]).reset_index(drop=True)\n",
    "del not_in_mapping_list, add_to_list\n",
    "seoul_link_mapping = seoul_link_mapping[seoul_link_mapping['서비스링크'].isin(service_link_list_from_speed)]\n",
    "del service_link_list_from_speed\n",
    "seoul_link_mapping.sort_values(by=['서비스링크', '표준링크아이디'], inplace=True)\n",
    "seoul_link_mapping.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Set seoul data set\n",
    "seoul_link = metropolitan_link[metropolitan_link['LINK_ID'].isin(seoul_link_mapping['표준링크아이디'].values)].copy()\n",
    "seoul_link.sort_values(by=['LINK_ID'], inplace=True)\n",
    "seoul_link.reset_index(drop=True, inplace=True)\n",
    "seoul_link = pd.merge(seoul_link, seoul_link_mapping, how='inner', left_on=['LINK_ID'], right_on=['표준링크아이디'])\n",
    "service_link_list = seoul_link['서비스링크'].unique()\n",
    "service_link_list.sort()\n",
    "print(f'The number of service link is {len(service_link_list)}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 2. Check number of nan in links (cost time, do not run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Used to check the number of nan\n",
    "nan_counts = np.zeros(len(service_link_list))\n",
    "\n",
    "for i, file in enumerate(listdir(os.getcwd()+'/seoul_speed_raw_data')):\n",
    "    if i>=12:\n",
    "        speed_data = pd.read_csv('./seoul_speed_raw_data/'+file)\n",
    "        speed_data['PRCS_SPD'] = speed_data['PRCS_SPD'].astype('int64')\n",
    "        speed_data['PRCS_SPD'].replace(0, np.nan, inplace=True)\n",
    "        date_list = speed_data['PRCS_DAY'].unique()\n",
    "        print(f'{file}')\n",
    "        print(f'The length of day is {len(date_list)}')\n",
    "        for idx, service_link in enumerate(service_link_list):\n",
    "            has_value = (~speed_data[speed_data['LINK_ID']==service_link]['PRCS_SPD'].isna()).sum()\n",
    "            if 12*24*len(date_list)-has_value!=0:\n",
    "                nan_counts[idx]+=(12*24*len(date_list)-has_value)\n",
    "\n",
    "for count, service_link in zip(nan_counts, service_link_list):\n",
    "    if count/(24*12*61)>0.10:\n",
    "        print(service_link, count/(24*12*61))\n",
    "\n",
    "del count, file, has_value, idx, nan_counts, service_link, speed_data, date_list\n",
    "\n",
    "# Missing more than 10% were excluded\n",
    "print('print end')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 3 Exclude to many missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the missing links\n",
    "nan_list = [1050048100, 1179910010, 1179920110, 2180000101, 2180000201, 2180002102, 2180002201]\n",
    "\n",
    "mask = np.in1d(service_link_list, nan_list, invert=True)\n",
    "service_link_list = service_link_list[mask]\n",
    "print(f'The number of service links in service_link_list = {len(service_link_list)}')\n",
    "\n",
    "seoul_link = seoul_link[~seoul_link['서비스링크'].isin(nan_list)]\n",
    "seoul_link.sort_values(by=['LINK_ID'], inplace=True)\n",
    "seoul_link.reset_index(drop=True, inplace=True)\n",
    "del nan_list\n",
    "seoul_link_restore = seoul_link.copy()\n",
    "\n",
    "print(f'The number of links = {len(seoul_link)}')\n",
    "print(f'The number of service links in seoul_link = {len(seoul_link[\"서비스링크\"].unique())}')\n",
    "\n",
    "crs = seoul_link.crs # Must store the original crs    \n",
    "\n",
    "# Change name to avoid error\n",
    "seoul_link = seoul_link.rename(columns={'서비스링크': 'SERVICE_ID','표준링크아이디': 'STD_ID'})\n",
    "seoul_link_restore = seoul_link_restore.rename(columns={'서비스링크': 'SERVICE_ID','표준링크아이디': 'STD_ID'})\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Reconnecting links"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 1. Reconnect 2-hop missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Skip this stage if data does not need to be restored\n",
    "for iteration in range(2):\n",
    "    seoul_node_list = np.union1d(np.unique(seoul_link_restore['F_NODE'].values), np.unique(seoul_link_restore['T_NODE'].values))\n",
    "\n",
    "    # The link which is not contained in seoul_link_restore\n",
    "    target_link = metropolitan_link[~metropolitan_link['LINK_ID'].isin(seoul_link_restore['LINK_ID'].values)]\n",
    "\n",
    "    # But has F_NODE and T_NODE inside seoul link\n",
    "    target_link = target_link[\n",
    "        target_link['F_NODE'].isin(seoul_node_list) &\n",
    "        target_link['T_NODE'].isin(seoul_node_list)]\n",
    "    target_link.sort_values(by=['LINK_ID'])\n",
    "    target_link.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    print(f'Before reconnecting the 1-hop missing, the length of seoul link is {len(seoul_link_restore)}')\n",
    "    print(f'Number of candidate link {len(target_link)}')\n",
    "\n",
    "    # Reconnect 1-hop per iteration\n",
    "    for i in tqdm(range(len(target_link))):\n",
    "        t_node = target_link.iloc[i]['T_NODE']\n",
    "        f_node = target_link.iloc[i]['T_NODE']\n",
    "        link_id_div = target_link.iloc[i]['LINK_ID']/1000\n",
    "        matches = seoul_link_restore[\n",
    "            (seoul_link_restore['F_NODE'] == t_node) &\n",
    "            (~(seoul_link_restore['T_NODE'] == f_node)) &\n",
    "            (seoul_link_restore['LINK_ID']/1000 == link_id_div)]\n",
    "\n",
    "        if len(matches)==0:\n",
    "            matches = seoul_link_restore[\n",
    "                (seoul_link_restore['F_NODE'] == t_node) &\n",
    "                (~(seoul_link_restore['T_NODE'] == f_node))]\n",
    "\n",
    "        added_link = target_link.iloc[i].copy()\n",
    "        if len(matches)>0:\n",
    "            matches.reset_index(drop=True, inplace=True)\n",
    "            added_link['SERVICE_ID'] = matches.iloc[0]['SERVICE_ID']\n",
    "            added_link['STD_ID'] = added_link['LINK_ID']\n",
    "\n",
    "            seoul_link_restore = pd.concat([seoul_link_restore, added_link.to_frame().transpose()])\n",
    "\n",
    "    seoul_link_restore.reset_index(drop=True, inplace=True)\n",
    "    print(f'After reconnecting the 1-hop missing, the length of seoul link is {len(seoul_link_restore)}')\n",
    "del t_node, f_node, link_id_div, matches, added_link, iteration, \n",
    "seoul_link_restore.crs = crs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Multi-hop reconnecting of major links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Before reconnecting the multi-hop missing, the length of seoul link is {len(seoul_link_restore)}')\n",
    "for iteration in range(3):\n",
    "    target_link = metropolitan_link[\n",
    "        (~metropolitan_link['LINK_ID'].isin(seoul_link_restore['LINK_ID'].values)) &\n",
    "        (metropolitan_link['ROAD_NAME'].isin([\n",
    "            '강변북로', '올림픽대로', '동부간선도로', '강남대로', '경부고속도로', '선유로', '노들로', '강변역로'\n",
    "            '청파로', '성산로', '원효로', '내부순환로', '강동대로', '반포대로', '녹사평대로', '여의동로', '여의서로',\n",
    "            '녹사평대로11길', '왕십리로', '봉은사로113길']))]\n",
    "\n",
    "    target_link.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    for i in (range(len(target_link))):\n",
    "        t_node = target_link.iloc[i]['T_NODE']\n",
    "        f_node = target_link.iloc[i]['T_NODE']\n",
    "        road_name = target_link.iloc[i]['ROAD_NAME']\n",
    "        matches = seoul_link_restore[\n",
    "            (seoul_link_restore['F_NODE'] == t_node) &\n",
    "            (~(seoul_link_restore['T_NODE'] == f_node)) &\n",
    "            (seoul_link_restore['ROAD_NAME'] == road_name)]\n",
    "        \n",
    "        if len(matches)==0:\n",
    "            matches = seoul_link_restore[\n",
    "                (seoul_link_restore['F_NODE'] == t_node) &\n",
    "                (~(seoul_link_restore['T_NODE'] == f_node))]\n",
    "\n",
    "        if len(matches)==0:\n",
    "            matches = seoul_link_restore[\n",
    "                (seoul_link_restore['F_NODE'] == f_node) |\n",
    "                (seoul_link_restore['T_NODE'] == t_node)]\n",
    "\n",
    "        try:\n",
    "            added_link = target_link.iloc[i].copy()\n",
    "            matches.reset_index(drop=True, inplace=True)\n",
    "            added_link['SERVICE_ID'] = matches.iloc[0]['SERVICE_ID']\n",
    "            added_link['STD_ID'] = added_link['LINK_ID']\n",
    "\n",
    "            seoul_link_restore = pd.concat([seoul_link_restore, added_link.to_frame().transpose()])\n",
    "            seoul_link_restore.reset_index(drop=True, inplace=True)\n",
    "        except: pass\n",
    "\n",
    "print(f'After reconnecting the 1st multi-hop missing, the length of seoul link is {len(seoul_link_restore)}')\n",
    "\n",
    "for iteration in range(4):\n",
    "    target_link = metropolitan_link[\n",
    "        (~metropolitan_link['LINK_ID'].isin(seoul_link_restore['LINK_ID'].values)) &\n",
    "        (metropolitan_link['ROAD_NAME'].isin(['올림픽대로', '강변역로', '현충로', '이촌로', '이촌로2길', '청파로', '동작대로', '강변북로']))\n",
    "        ]\n",
    "\n",
    "    target_link.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    for i in (range(len(target_link))):\n",
    "        t_node = target_link.iloc[i]['T_NODE']\n",
    "        f_node = target_link.iloc[i]['T_NODE']\n",
    "        road_name = target_link.iloc[i]['ROAD_NAME']\n",
    "        matches = seoul_link_restore[\n",
    "            (seoul_link_restore['F_NODE'] == t_node) &\n",
    "            (~(seoul_link_restore['T_NODE'] == f_node)) &\n",
    "            (seoul_link_restore['ROAD_NAME'] == road_name)]\n",
    "        \n",
    "        if len(matches)==0:\n",
    "            matches = seoul_link_restore[\n",
    "                (seoul_link_restore['F_NODE'] == t_node) &\n",
    "                (~(seoul_link_restore['T_NODE'] == f_node))]\n",
    "\n",
    "        if len(matches)==0:\n",
    "            matches = seoul_link_restore[\n",
    "                (seoul_link_restore['F_NODE'] == f_node) |\n",
    "                (seoul_link_restore['T_NODE'] == t_node)]\n",
    "\n",
    "        try:\n",
    "            added_link = target_link.iloc[i].copy()\n",
    "            matches.reset_index(drop=True, inplace=True)\n",
    "            added_link['SERVICE_ID'] = matches.iloc[0]['SERVICE_ID']\n",
    "            added_link['STD_ID'] = added_link['LINK_ID']\n",
    "\n",
    "            seoul_link_restore = pd.concat([seoul_link_restore, added_link.to_frame().transpose()])\n",
    "            seoul_link_restore.reset_index(drop=True, inplace=True)\n",
    "        except: pass\n",
    "\n",
    "print(f'After reconnecting the 2nd multi-hop missing, the length of seoul link is {len(seoul_link_restore)}')\n",
    "del t_node, f_node, matches, added_link, i, iteration, road_name, target_link\n",
    "seoul_link_restore.crs = crs\n",
    "\n",
    "print(f'The total number of service link in seoul raw is {len(seoul_link[\"SERVICE_ID\"].unique())}')\n",
    "print(f'The total number of service link in seoul restore is {len(seoul_link_restore[\"SERVICE_ID\"].unique())}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 3. Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add restore to the end of the filename if the mode is restore\n",
    "\n",
    "seoul_link_restore.sort_values(by=['SERVICE_ID', 'LINK_ID'], inplace=True)\n",
    "seoul_link_restore.reset_index(drop=True, inplace=True)\n",
    "\n",
    "seoul_node_list_restore = np.union1d(np.unique(seoul_link_restore['F_NODE'].values), np.unique(seoul_link_restore['T_NODE'].values))\n",
    "seoul_node_restore = metropolitan_node[metropolitan_node['NODE_ID'].isin(seoul_node_list_restore)]\n",
    "seoul_node_restore.sort_values(by=['NODE_ID'])\n",
    "seoul_node_restore.reset_index(drop=True, inplace=True)\n",
    "\n",
    "seoul_node_restore.to_file(\"seoul_node_restore.shp\", encoding='utf-8')\n",
    "\n",
    "seoul_link_restore = seoul_link_restore.apply(pd.to_numeric, errors='ignore')\n",
    "seoul_link_restore.to_file(\"seoul_link_restore.shp\", encoding='utf-8')\n",
    "\n",
    "seoul_link_restore.to_csv('seoul_link_restore.csv', encoding='euc-kr', index=False)\n",
    "seoul_node_restore.to_csv('seoul_node_restore.csv', encoding='euc-kr', index=False)\n",
    "\n",
    "seoul_link.sort_values(by=['SERVICE_ID', 'LINK_ID'], inplace=True)\n",
    "seoul_link.reset_index(drop=True, inplace=True)\n",
    "\n",
    "seoul_node_list = np.union1d(np.unique(seoul_link['F_NODE'].values), np.unique(seoul_link['T_NODE'].values))\n",
    "seoul_node = metropolitan_node[metropolitan_node['NODE_ID'].isin(seoul_node_list)]\n",
    "seoul_node.sort_values(by=['NODE_ID'])\n",
    "seoul_node.reset_index(drop=True, inplace=True)\n",
    "\n",
    "seoul_node.to_file(\"seoul_node_raw.shp\", encoding='utf-8')\n",
    "\n",
    "seoul_link = seoul_link.apply(pd.to_numeric, errors='ignore')\n",
    "seoul_link.to_file(\"seoul_link_raw.shp\", encoding='utf-8')\n",
    "\n",
    "seoul_link.to_csv('seoul_link_raw.csv', encoding='euc-kr', index=False)\n",
    "seoul_node.to_csv('seoul_node_raw.csv', encoding='euc-kr', index=False)\n",
    "\n",
    "del metropolitan_link, metropolitan_node, seoul_link_mapping, seoul_node_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이 사이에 restore 파일에서 서울시 밖으로 과도하게 확장된 링크를 자르고 그 링크를 seoul_link_restore라고 저장\n",
    "# 참고로 그 서울시 파일의 좌표계는 5179이며, 손으로 직접 폴리곤을 그려서 잘라야 한다. 절묘하게 튀어나와 있는 링크들이 가끔 있기 때문\n",
    "\n",
    "seoul_link_restore = geopandas.read_file('seoul_link_restore.shp', encoding='utf-8')\n",
    "\n",
    "seoul_link_restore.sort_values(by=['SERVICE_ID', 'LINK_ID'], inplace=True)\n",
    "seoul_link_restore.reset_index(drop=True, inplace=True)\n",
    "\n",
    "seoul_node_restore_list = np.union1d(np.unique(seoul_link_restore['F_NODE'].values), np.unique(seoul_link_restore['T_NODE'].values))\n",
    "\n",
    "metropolitan_node = geopandas.read_file('[2022-09-05]NODELINKDATA/metropolitan_node.shp', encoding='utf-8')\n",
    "metropolitan_node = metropolitan_node.apply(pd.to_numeric, errors='ignore')\n",
    "metropolitan_node.sort_values(by=['NODE_ID'], inplace=True)\n",
    "\n",
    "seoul_node_restore = metropolitan_node[metropolitan_node['NODE_ID'].isin(seoul_node_restore_list)]\n",
    "seoul_node_restore.sort_values(by=['NODE_ID'])\n",
    "seoul_node_restore.reset_index(drop=True, inplace=True)\n",
    "\n",
    "seoul_node_restore.to_file(\"seoul_node_restore.shp\", encoding='utf-8')\n",
    "\n",
    "seoul_link_restore = seoul_link_restore.apply(pd.to_numeric, errors='ignore')\n",
    "seoul_link_restore.to_file(\"seoul_link_restore.shp\", encoding='utf-8')\n",
    "\n",
    "seoul_link_restore.to_csv('seoul_link_restore.csv', encoding='euc-kr', index=False)\n",
    "seoul_node_restore.to_csv('seoul_node_restore.csv', encoding='euc-kr', index=False)\n",
    "\n",
    "print(f'The total number of service link in seoul raw is {len(seoul_link[\"SERVICE_ID\"].unique())}')\n",
    "print(f'The total number of service link in seoul restore is {len(seoul_link_restore[\"SERVICE_ID\"].unique())}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Create speed dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from os import listdir\n",
    "import itertools\n",
    "from tqdm import tqdm \n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import geopandas"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. 1. Load and merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_data_type(df, datetime=True):\n",
    "    df['PRCS_YEAR'] = df['PRCS_YEAR'].astype('int16')\n",
    "    df['PRCS_MON'] = df['PRCS_MON'].astype('int8')\n",
    "    df['PRCS_DAY'] = df['PRCS_DAY'].astype('int8')\n",
    "    df['PRCS_HH'] = df['PRCS_HH'].astype('int8')\n",
    "    df['PRCS_MIN'] = df['PRCS_MIN'].astype('int8')\n",
    "    df['LINK_ID'] = df['LINK_ID'].astype('uint32')\n",
    "    \n",
    "    try: \n",
    "        df['PRCS_SPD'].replace(0, np.nan, inplace=True)\n",
    "        df['PRCS_SPD'] = df['PRCS_SPD'].astype('float16')\n",
    "    except: pass\n",
    "    \n",
    "    if datetime==True:\n",
    "        df['PRCS_DATETIME'] = (\n",
    "            df['PRCS_YEAR'].map(str)+df['PRCS_MON'].map(str).str.zfill(2)+df['PRCS_DAY'].map(str).str.zfill(2)\n",
    "            +df['PRCS_HH'].map(str).str.zfill(2)+df['PRCS_MIN'].map(str).str.zfill(2))\n",
    "        df['PRCS_DATETIME'] = pd.to_datetime(df['PRCS_DATETIME'],format='%Y%m%d%H%M')\n",
    "    else: df['PRCS_DATETIME'] = pd.to_datetime(df['PRCS_DATETIME'])\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seoul_link = geopandas.read_file('seoul_link_raw.shp', encoding='utf-8')\n",
    "seoul_link.sort_values(by=['SERVICE_ID', 'STD_ID'], inplace=True)\n",
    "seoul_link.reset_index(drop=True, inplace=True)\n",
    "service_link_list = seoul_link['SERVICE_ID'].unique()\n",
    "service_link_list.sort()\n",
    "\n",
    "for i, file in enumerate(listdir(os.getcwd()+'/seoul_speed_raw_data')):\n",
    "    if i>=12:\n",
    "        speed_data = pd.read_csv('./seoul_speed_raw_data/'+file)\n",
    "        speed_data = speed_data[speed_data['LINK_ID'].isin(service_link_list)]\n",
    "        speed_data = change_data_type(speed_data)\n",
    "\n",
    "        if (i<13): file_concat = pd.DataFrame(columns=speed_data.columns)\n",
    "\n",
    "        month, start_day, end_day = int(file[5:7]), int(file[7:9]), int(file[12:14])\n",
    "        time_df = pd.DataFrame(list(itertools.product([2022], [month], range(start_day, end_day+1), range(0,24), range(0,60,5), service_link_list)), columns=speed_data.columns[:-2])\n",
    "        time_df = change_data_type(time_df)\n",
    "\n",
    "        time_df = pd.merge(\n",
    "            time_df, speed_data[['PRCS_DATETIME', 'LINK_ID', 'PRCS_SPD']], how='left',\n",
    "            left_on=['PRCS_DATETIME', 'LINK_ID'],  right_on=['PRCS_DATETIME', 'LINK_ID'])\n",
    "\n",
    "        file_concat = pd.concat([file_concat, time_df])\n",
    "\n",
    "        print(f'month = {file[5:7]}, day = {file[7:9]}, day = {file[12:14]}')\n",
    "\n",
    "file_concat = change_data_type(file_concat, datetime=False)\n",
    "file_concat.sort_values(by=['LINK_ID', 'PRCS_DATETIME'], inplace=True)\n",
    "file_concat.reset_index(drop=True, inplace=True)\n",
    "print(file_concat.dtypes)\n",
    "\n",
    "print(file_concat['PRCS_SPD'].isna().sum())\n",
    "\n",
    "del end_day, file, i, month, start_day, time_df, speed_data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. 2. Interpolate nan values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Has nan value at October 2nd, 04:50. Interpolate by LINK_ID-PRCS_YEAR-PRCS_MON-PRCS_DAY-PRCS_HH...\n",
    "\n",
    "print(file_concat['PRCS_SPD'].isna().sum())\n",
    "\n",
    "for iteration in range(5):\n",
    "    # Interpolate by LINK_ID - TIME\n",
    "    print(f'sort values by {list([file_concat.columns.values[-3]])+list([file_concat.columns.values[-1]])}')\n",
    "    file_concat.sort_values(by=['LINK_ID', 'PRCS_DATETIME'], inplace=True)\n",
    "    file_concat.reset_index(drop=True, inplace=True)\n",
    "    file_concat['PRCS_SPD'].interpolate(limit=12, inplace=True)\n",
    "    print(file_concat['PRCS_SPD'].isna().sum())\n",
    "\n",
    "    # Interpolate by TIME - LINK_ID\n",
    "    print(f'sort values by {list([file_concat.columns.values[-1]])+list([file_concat.columns.values[-3]])}')\n",
    "    file_concat.sort_values(by=['PRCS_DATETIME', 'LINK_ID'], inplace=True)\n",
    "    file_concat.reset_index(drop=True, inplace=True)\n",
    "    file_concat['PRCS_SPD'].interpolate(limit=4, inplace=True)\n",
    "    print(file_concat['PRCS_SPD'].isna().sum())\n",
    "\n",
    "file_concat.to_csv('seoul_speed.csv', encoding='utf-8', index=False)\n",
    "\n",
    "del file_concat"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Create adjacency matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import geopandas\n",
    "\n",
    "seoul_link_restore = geopandas.read_file('seoul_link_restore.shp', encoding='utf-8')\n",
    "seoul_link_restore.sort_values(by=['SERVICE_ID', 'STD_ID'], inplace=True)\n",
    "seoul_link_restore.reset_index(drop=True, inplace=True)\n",
    "service_link_list = seoul_link_restore['SERVICE_ID'].unique()\n",
    "service_link_list.sort()\n",
    "\n",
    "seoul_link_restore_df = pd.DataFrame(columns=[\n",
    "  'SERVICE_ID', 'F_NODE', 'T_NODE', 'NAME','LENGTH',\n",
    "  'AVG_LANES', 'MIN_LANES', 'MAX_LANES', 'LANE_CHANGE'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. 1. Store link info - with restored links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, idx in enumerate(service_link_list):\n",
    "  tmp_seoul_link_restore = seoul_link_restore[seoul_link_restore['SERVICE_ID'] == idx]\n",
    "\n",
    "  # Min/Max/Average number of lanes link length for feature\n",
    "  min_lanes, max_lanes = tmp_seoul_link_restore['LANES'].min(), tmp_seoul_link_restore['LANES'].max()\n",
    "  link_length = round(tmp_seoul_link_restore['LENGTH'].sum(), 4)\n",
    "  area = (tmp_seoul_link_restore['LANES']*tmp_seoul_link_restore['LENGTH']).sum()\n",
    "  avg_lanes = round(area/link_length, 4)\n",
    "\n",
    "  # Get a list of starting node (F NODE) ​​and ending node (T NODE) ​​of a service link\n",
    "  f_node = tmp_seoul_link_restore['F_NODE'].unique()\n",
    "  t_node = tmp_seoul_link_restore['T_NODE'].unique()\n",
    "\n",
    "  f_node.sort()\n",
    "  t_node.sort()\n",
    "\n",
    "  tmp_list = [idx, f_node, t_node, tmp_seoul_link_restore.iloc[0]['ROAD_NAME'],\n",
    "              link_length, avg_lanes, min_lanes, max_lanes, min_lanes!=max_lanes]\n",
    "\n",
    "  seoul_link_restore_df.loc[i] = tmp_list\n",
    "\n",
    "  del area, avg_lanes, f_node, i, idx, link_length, max_lanes, min_lanes, t_node, tmp_list, tmp_seoul_link_restore\n",
    "\n",
    "seoul_link_restore_df.sort_values(by=['SERVICE_ID'], inplace=True)\n",
    "seoul_link_restore_df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. 2. Create adjacency matrix - with restored links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To remove the non-existing U-turn, need to subtract when (F_NODE list==T_NODE list) and (T_NODE list==F_NODE list)\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "forward_a1 = pd.DataFrame(columns = service_link_list, index = service_link_list)\n",
    "backward_a1 = pd.DataFrame(columns = service_link_list, index = service_link_list)\n",
    "total_a1 = pd.DataFrame(columns = service_link_list, index = service_link_list)\n",
    "\n",
    "seoul_link_restore_df_f_node = pd.DataFrame(columns=['F_NODE']*25, index = service_link_list)\n",
    "seoul_link_restore_df_t_node = pd.DataFrame(columns=['T_NODE']*25, index = service_link_list)\n",
    "\n",
    "for idx in forward_a1.index:\n",
    "  tmp_df = seoul_link_restore_df[seoul_link_restore_df['SERVICE_ID']==idx]\n",
    "  f_node_list, t_node_list = tmp_df['F_NODE'].values[0], tmp_df['T_NODE'].values[0]\n",
    "\n",
    "  for i, f_node in enumerate(f_node_list):\n",
    "    seoul_link_restore_df_f_node.loc[idx][i] = f_node\n",
    "  for i, t_node in enumerate(t_node_list):\n",
    "    seoul_link_restore_df_t_node.loc[idx][i] = t_node\n",
    "\n",
    "for idx in forward_a1.index:\n",
    "  tmp_df = seoul_link_restore_df[seoul_link_restore_df['SERVICE_ID']==idx]\n",
    "  f_node_list, t_node_list = tmp_df['F_NODE'].values[0], tmp_df['T_NODE'].values[0]\n",
    "\n",
    "  # Add to forward list if my t node is on f node of a specific link, and vice versa. Addition for a row has a value greater than 0 if there is an element\n",
    "  forward_list = seoul_link_restore_df_f_node.index[seoul_link_restore_df_f_node['F_NODE'].isin(t_node_list).sum(axis=1)>0]\n",
    "  backward_list = seoul_link_restore_df_t_node.index[seoul_link_restore_df_t_node['T_NODE'].isin(f_node_list).sum(axis=1)>0]\n",
    "  total_list = np.union1d(forward_list, backward_list)\n",
    "\n",
    "  forward_a1.loc[idx][forward_list] = 1\n",
    "  backward_a1.loc[idx][backward_list] = 1\n",
    "  total_a1.loc[idx][total_list] = 1\n",
    "\n",
    "  for forward_link in forward_list:\n",
    "    forward_link_f_node = seoul_link_restore_df_f_node.loc[forward_link][seoul_link_restore_df_f_node.loc[forward_link].notnull()].values\n",
    "    forward_link_t_node = seoul_link_restore_df_t_node.loc[forward_link][seoul_link_restore_df_t_node.loc[forward_link].notnull()].values\n",
    "    \n",
    "    forward_link_f_diff = set(forward_link_f_node) - set(forward_link_t_node)\n",
    "    forward_link_t_diff = set(forward_link_t_node) - set(forward_link_f_node)\n",
    "    f_node_list_diff = set(f_node_list) - set(t_node_list)\n",
    "    t_node_list_diff = set(t_node_list) - set(f_node_list)\n",
    "    \n",
    "    forward_link_f_node_t_node_list_diff = set(forward_link_f_node) - set(t_node_list)\n",
    "    forward_link_t_node_f_node_list_diff = set(forward_link_t_node) - set(f_node_list)\n",
    "\n",
    "    f_node_list_forward_link_t_node_diff = set(f_node_list) - set(forward_link_t_node)\n",
    "    t_node_list_forward_link_f_node_diff = set(t_node_list) - set(forward_link_f_node)\n",
    "\n",
    "    len_1 = (len(t_node_list_forward_link_f_node_diff)+len(f_node_list_forward_link_t_node_diff))\n",
    "    len_2 = (len(forward_link_f_node_t_node_list_diff)+len(forward_link_t_node_f_node_list_diff))\n",
    "\n",
    "    # Delete self-loop\n",
    "    if forward_link == idx:\n",
    "      forward_a1.loc[idx][forward_link] = 0\n",
    "      backward_a1.loc[idx][forward_link] = 0\n",
    "      total_a1.loc[idx][forward_link] = 0\n",
    "\n",
    "    # Delete non-existing u-turn\n",
    "    elif ((set(forward_link_t_node) == set(f_node_list)) & (set(forward_link_f_node) == set(t_node_list))):\n",
    "      forward_a1.loc[idx][forward_link] = 0\n",
    "      backward_a1.loc[idx][forward_link] = 0\n",
    "      total_a1.loc[idx][forward_link] = 0\n",
    "\n",
    "    elif ((forward_link_f_diff == t_node_list_diff) & (forward_link_t_diff == f_node_list_diff)):\n",
    "      forward_a1.loc[idx][forward_link] = 0\n",
    "      backward_a1.loc[idx][forward_link] = 0\n",
    "      total_a1.loc[idx][forward_link] = 0\n",
    "\n",
    "    elif ((len_1==0) or (len_2==0)):\n",
    "      forward_a1.loc[idx][forward_link] = 0\n",
    "      backward_a1.loc[idx][forward_link] = 0\n",
    "      total_a1.loc[idx][forward_link] = 0\n",
    "\n",
    "del tmp_df, f_node, f_node_list, t_node, t_node_list, idx, i, forward_list, backward_list, total_list, forward_link_f_node, forward_link_t_node, forward_link"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. 3. Store link info - with raw links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "seoul_link_raw = geopandas.read_file('seoul_link_raw.shp', encoding='utf-8')\n",
    "seoul_link_raw.sort_values(by=['SERVICE_ID', 'STD_ID'], inplace=True)\n",
    "seoul_link_raw.reset_index(drop=True, inplace=True)\n",
    "service_link_list = seoul_link_raw['SERVICE_ID'].unique()\n",
    "service_link_list.sort()\n",
    "\n",
    "seoul_link_raw_df = pd.DataFrame(columns=[\n",
    "  'SERVICE_ID', 'F_NODE', 'T_NODE', 'NAME','LENGTH',\n",
    "  'AVG_LANES', 'MIN_LANES', 'MAX_LANES', 'LANE_CHANGE'])\n",
    "\n",
    "for i, idx in enumerate(service_link_list):\n",
    "  tmp_seoul_link_raw = seoul_link_raw[seoul_link_raw['SERVICE_ID'] == idx]\n",
    "\n",
    "  # Min/Max/Average number of lanes link length for feature\n",
    "  min_lanes, max_lanes = tmp_seoul_link_raw['LANES'].min(), tmp_seoul_link_raw['LANES'].max()\n",
    "  link_length = round(tmp_seoul_link_raw['LENGTH'].sum(), 4)\n",
    "  area = (tmp_seoul_link_raw['LANES']*tmp_seoul_link_raw['LENGTH']).sum()\n",
    "  avg_lanes = round(area/link_length, 4)\n",
    "\n",
    "  # Get a list of starting node (F NODE) ​​and ending node (T NODE) ​​of a service link\n",
    "  f_node = tmp_seoul_link_raw['F_NODE'].unique()\n",
    "  t_node = tmp_seoul_link_raw['T_NODE'].unique()\n",
    "\n",
    "  f_node.sort()\n",
    "  t_node.sort()\n",
    "\n",
    "  tmp_list = [idx, f_node, t_node, tmp_seoul_link_raw.iloc[0]['ROAD_NAME'],\n",
    "              link_length, avg_lanes, min_lanes, max_lanes, min_lanes!=max_lanes]\n",
    "\n",
    "  seoul_link_raw_df.loc[i] = tmp_list\n",
    "\n",
    "  del area, avg_lanes, f_node, i, idx, link_length, max_lanes, min_lanes, t_node, tmp_list, tmp_seoul_link_raw\n",
    "\n",
    "seoul_link_raw_df.sort_values(by=['SERVICE_ID'], inplace=True)\n",
    "seoul_link_raw_df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. 4. Add adjacency matris - with raw links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To remove the non-existing U-turn, need to subtract when (F_NODE list==T_NODE list) and (T_NODE list==F_NODE list)\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "seoul_link_raw_df_f_node = pd.DataFrame(columns=['F_NODE']*25, index = service_link_list)\n",
    "seoul_link_raw_df_t_node = pd.DataFrame(columns=['T_NODE']*25, index = service_link_list)\n",
    "\n",
    "for idx in forward_a1.index:\n",
    "  tmp_df = seoul_link_raw_df[seoul_link_raw_df['SERVICE_ID']==idx]\n",
    "  f_node_list, t_node_list = tmp_df['F_NODE'].values[0], tmp_df['T_NODE'].values[0]\n",
    "\n",
    "  for i, f_node in enumerate(f_node_list):\n",
    "    seoul_link_raw_df_f_node.loc[idx][i] = f_node\n",
    "  for i, t_node in enumerate(t_node_list):\n",
    "    seoul_link_raw_df_t_node.loc[idx][i] = t_node\n",
    "\n",
    "for idx in forward_a1.index:\n",
    "  tmp_df = seoul_link_raw_df[seoul_link_raw_df['SERVICE_ID']==idx]\n",
    "  f_node_list, t_node_list = tmp_df['F_NODE'].values[0], tmp_df['T_NODE'].values[0]\n",
    "\n",
    "  # Add to forward list if my t node is on f node of a specific link, and vice versa. Addition for a row has a value greater than 0 if there is an element\n",
    "  forward_list = seoul_link_raw_df_f_node.index[seoul_link_raw_df_f_node['F_NODE'].isin(t_node_list).sum(axis=1)>0]\n",
    "  backward_list = seoul_link_raw_df_t_node.index[seoul_link_raw_df_t_node['T_NODE'].isin(f_node_list).sum(axis=1)>0]\n",
    "  total_list = np.union1d(forward_list, backward_list)\n",
    "\n",
    "  forward_a1.loc[idx][forward_list] = 1\n",
    "  backward_a1.loc[idx][backward_list] = 1\n",
    "  total_a1.loc[idx][total_list] = 1\n",
    "\n",
    "  for forward_link in forward_list:\n",
    "    forward_link_f_node = seoul_link_raw_df_f_node.loc[forward_link][seoul_link_raw_df_f_node.loc[forward_link].notnull()].values\n",
    "    forward_link_t_node = seoul_link_raw_df_t_node.loc[forward_link][seoul_link_raw_df_t_node.loc[forward_link].notnull()].values\n",
    "    \n",
    "    forward_link_f_diff = set(forward_link_f_node) - set(forward_link_t_node)\n",
    "    forward_link_t_diff = set(forward_link_t_node) - set(forward_link_f_node)\n",
    "    f_node_list_diff = set(f_node_list) - set(t_node_list)\n",
    "    t_node_list_diff = set(t_node_list) - set(f_node_list)\n",
    "\n",
    "    # Delete self-loop\n",
    "    if forward_link == idx:\n",
    "      forward_a1.loc[idx][forward_link] = 0\n",
    "      backward_a1.loc[idx][forward_link] = 0\n",
    "      total_a1.loc[idx][forward_link] = 0\n",
    "\n",
    "    # Delete non-existing u-turn\n",
    "    elif ((set(forward_link_t_node) == set(f_node_list)) & (set(forward_link_f_node) == set(t_node_list))):\n",
    "      forward_a1.loc[idx][forward_link] = 0\n",
    "      backward_a1.loc[idx][forward_link] = 0\n",
    "      total_a1.loc[idx][forward_link] = 0\n",
    "\n",
    "    elif ((forward_link_f_diff == t_node_list_diff) & (forward_link_t_diff == f_node_list_diff)):\n",
    "      forward_a1.loc[idx][forward_link] = 0\n",
    "      backward_a1.loc[idx][forward_link] = 0\n",
    "      total_a1.loc[idx][forward_link] = 0\n",
    "\n",
    "del tmp_df, f_node, f_node_list, t_node, t_node_list, idx, i, forward_list, backward_list, total_list, forward_link_f_node, forward_link_t_node, forward_link"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. 5. Finalize adjacency matrix with turninfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reflect turninfo\n",
    "turninfo = pd.read_csv('[2022-09-05]NODELINKDATA/TURNINFO.csv', encoding='utf-8')\n",
    "\n",
    "turninfo['ST_LINK'] = turninfo['ST_LINK'].astype('int64')\n",
    "turninfo['ED_LINK'] = turninfo['ED_LINK'].astype('int64')\n",
    "turninfo['TURN_TYPE'] = turninfo['TURN_TYPE'].astype('int64')\n",
    "\n",
    "turninfo = turninfo[\n",
    "    (turninfo['ST_LINK'].isin(seoul_link_restore['STD_ID'].unique())) &\n",
    "    (turninfo['ED_LINK'].isin(seoul_link_restore['STD_ID'].unique()))]\n",
    "\n",
    "turninfo.reset_index(drop=True, inplace=True)\n",
    "\n",
    "for idx in turninfo.index:\n",
    "    turninfo_tmp = turninfo.loc[idx]\n",
    "    st_link = seoul_link_restore[seoul_link_restore['STD_ID']==turninfo_tmp['ST_LINK']]['SERVICE_ID'].values[0]\n",
    "    ed_link = seoul_link_restore[seoul_link_restore['STD_ID']==turninfo_tmp['ED_LINK']]['SERVICE_ID'].values[0]\n",
    "    turntype = turninfo_tmp['TURN_TYPE']\n",
    "\n",
    "    if turntype in [2, 3, 101, 102, 103] :\n",
    "        forward_a1.loc[st_link][ed_link] = 0\n",
    "        backward_a1.loc[ed_link][st_link] = 0 \n",
    "        total_a1.loc[st_link][ed_link] = 0\n",
    "\n",
    "    elif turntype in [1, 11, 12] :\n",
    "        forward_a1.loc[st_link][ed_link] = 1\n",
    "        backward_a1.loc[ed_link][st_link] = 1\n",
    "        total_a1.loc[st_link][ed_link] = 1\n",
    "        total_a1.loc[ed_link][st_link] = 1\n",
    "\n",
    "forward_a1 = forward_a1.fillna(0)\n",
    "backward_a1 = backward_a1.fillna(0)\n",
    "total_a1 = total_a1.fillna(0)\n",
    "\n",
    "# Sort in ascending order of row and column\n",
    "forward_a1.sort_index(axis = 0, inplace=True)\n",
    "forward_a1.sort_index(axis = 1, inplace=True)\n",
    "backward_a1.sort_index(axis = 0, inplace=True)\n",
    "backward_a1.sort_index(axis = 1, inplace=True)\n",
    "total_a1.sort_index(axis = 0, inplace=True)\n",
    "total_a1.sort_index(axis = 1, inplace=True)\n",
    "\n",
    "arr = forward_a1.to_numpy()\n",
    "np.fill_diagonal(arr, 1)\n",
    "forward_a1 = pd.DataFrame(arr, columns=forward_a1.columns, index=forward_a1.index)\n",
    "arr = backward_a1.to_numpy()\n",
    "np.fill_diagonal(arr, 1)\n",
    "backward_a1 = pd.DataFrame(arr, columns=backward_a1.columns, index=backward_a1.index)\n",
    "arr = total_a1.to_numpy()\n",
    "np.fill_diagonal(arr, 1)\n",
    "total_a1 = pd.DataFrame(arr, columns=total_a1.columns, index=total_a1.index)\n",
    "\n",
    "del ed_link, st_link, turninfo_tmp, turntype, idx"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. 6. Save multiplication of a matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def matrix_multiplication(a_matrix, type):\n",
    "    multiplier = a_matrix\n",
    "    for i in range(1,10,1):\n",
    "        multiplier.to_csv(f'seoul_adjacency_matrix/{type}/seoul_{type}_a{i}.csv')\n",
    "        print(f'seoul_{type}_a{i}')\n",
    "        if i<9:\n",
    "            multiplier = multiplier@a_matrix\n",
    "        else: pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seoul_forward_a1\n",
      "seoul_forward_a2\n",
      "seoul_forward_a3\n",
      "seoul_forward_a4\n",
      "seoul_forward_a5\n",
      "seoul_forward_a6\n",
      "seoul_forward_a7\n",
      "seoul_forward_a8\n",
      "seoul_forward_a9\n",
      "seoul_backward_a1\n",
      "seoul_backward_a2\n",
      "seoul_backward_a3\n",
      "seoul_backward_a4\n",
      "seoul_backward_a5\n",
      "seoul_backward_a6\n",
      "seoul_backward_a7\n",
      "seoul_backward_a8\n",
      "seoul_backward_a9\n",
      "seoul_total_a1\n",
      "seoul_total_a2\n",
      "seoul_total_a3\n",
      "seoul_total_a4\n",
      "seoul_total_a5\n",
      "seoul_total_a6\n",
      "seoul_total_a7\n",
      "seoul_total_a8\n",
      "seoul_total_a9\n"
     ]
    }
   ],
   "source": [
    "matrix_multiplication(forward_a1, 'forward')\n",
    "matrix_multiplication(backward_a1, 'backward')\n",
    "matrix_multiplication(total_a1, 'total')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seoul_forward_a1\n",
      "1\n",
      "seoul_forward_a2\n",
      "7\n",
      "seoul_forward_a3\n",
      "24\n",
      "seoul_forward_a4\n",
      "94\n",
      "seoul_forward_a5\n",
      "412\n",
      "seoul_forward_a6\n",
      "1821\n",
      "seoul_forward_a7\n",
      "8107\n",
      "seoul_forward_a8\n",
      "36311\n",
      "seoul_forward_a9\n",
      "163404\n",
      "seoul_backward_a1\n",
      "1\n",
      "seoul_backward_a2\n",
      "7\n",
      "seoul_backward_a3\n",
      "24\n",
      "seoul_backward_a4\n",
      "94\n",
      "seoul_backward_a5\n",
      "412\n",
      "seoul_backward_a6\n",
      "1821\n",
      "seoul_backward_a7\n",
      "8107\n",
      "seoul_backward_a8\n",
      "36311\n",
      "seoul_backward_a9\n",
      "163404\n",
      "seoul_total_a1\n",
      "1\n",
      "seoul_total_a2\n",
      "15\n",
      "seoul_total_a3\n",
      "76\n",
      "seoul_total_a4\n",
      "586\n",
      "seoul_total_a5\n",
      "4325\n",
      "seoul_total_a6\n",
      "32814\n",
      "seoul_total_a7\n",
      "249253\n",
      "seoul_total_a8\n",
      "1910377\n",
      "seoul_total_a9\n",
      "14699616\n"
     ]
    }
   ],
   "source": [
    "# This code can be used to get a appropriate value of beta\n",
    "'''\n",
    "TODO: find a good beta\n",
    "'''\n",
    "import itertools\n",
    "for type, square_num in itertools.product(['forward', 'backward', 'total'], range(1,10,1)):\n",
    "    a_matrix = pd.read_csv(f'seoul_adjacency_matrix/{type}/seoul_{type}_a{square_num}.csv', index_col=0)\n",
    "    print(f'seoul_{type}_a{square_num}')\n",
    "    print((a_matrix.max()).max())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.7 Create shp file based on restore service link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import geopandas\n",
    "from shapely.ops import linemerge\n",
    "from shapely.ops import unary_union\n",
    "\n",
    "seoul_link_restore = geopandas.read_file('seoul_link_restore.shp', encoding='utf-8')\n",
    "seoul_link_restore.sort_values(by=['SERVICE_ID', 'STD_ID'], inplace=True)\n",
    "seoul_link_restore.reset_index(drop=True, inplace=True)\n",
    "service_link_list = seoul_link_restore['SERVICE_ID'].unique()\n",
    "service_link_list.sort()\n",
    "\n",
    "seoul_link_restore['LINK_ID'] = seoul_link_restore['LINK_ID'].astype('int64')\n",
    "seoul_link_restore['F_NODE'] = seoul_link_restore['F_NODE'].astype('int64')\n",
    "seoul_link_restore['T_NODE'] = seoul_link_restore['T_NODE'].astype('int64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "seoul_service_link_restore = geopandas.GeoDataFrame(columns=[\n",
    "  'SERVICE_ID', 'F_NODE', 'T_NODE', 'NAME','LENGTH',\n",
    "  'AVG_LANES', 'MIN_LANES', 'MAX_LANES', 'LANE_CHANGE', 'AREA', 'geometry'])\n",
    "\n",
    "for i, idx in enumerate(service_link_list):\n",
    "  tmp_seoul_link_restore = seoul_link_restore[seoul_link_restore['SERVICE_ID'] == idx]\n",
    "  tmp_seoul_link_restore.reset_index(drop=True, inplace=True)\n",
    "\n",
    "  # Min/max/mean lanes and length to use it as a feature\n",
    "  min_lanes, max_lanes = tmp_seoul_link_restore['LANES'].min(), tmp_seoul_link_restore['LANES'].max()\n",
    "  link_length = round(tmp_seoul_link_restore['LENGTH'].sum(), 4)\n",
    "  area = (tmp_seoul_link_restore['LANES']*tmp_seoul_link_restore['LENGTH']).sum()\n",
    "  avg_lanes = round(area/link_length, 4)\n",
    "\n",
    "  # Get ininitial node(F NODE) and terminal node(T NODE)\n",
    "  f_node = tmp_seoul_link_restore['F_NODE'].unique()\n",
    "  t_node = tmp_seoul_link_restore['T_NODE'].unique()\n",
    "\n",
    "  f_node.sort()\n",
    "  t_node.sort()\n",
    "  f_node = f_node.tolist()\n",
    "  t_node = t_node.tolist()\n",
    "  \n",
    "  merged_line = unary_union(tmp_seoul_link_restore['geometry'])\n",
    "  tmp_list = [idx, str(f_node)[1:-1], str(t_node)[1:-1], tmp_seoul_link_restore.iloc[0]['ROAD_NAME'],\n",
    "              link_length, avg_lanes, min_lanes, max_lanes, min_lanes!=max_lanes, area, merged_line]\n",
    "\n",
    "  seoul_service_link_restore.loc[i] = tmp_list\n",
    "\n",
    "crs = seoul_link_restore.crs\n",
    "seoul_service_link_restore.crs = crs\n",
    "\n",
    "seoul_service_link_restore.set_geometry(col='geometry', inplace=True, drop=True)\n",
    "seoul_service_link_restore = seoul_service_link_restore.apply(pd.to_numeric, errors='ignore')\n",
    "\n",
    "seoul_service_link_restore.to_file(\"seoul_service_link_restore.shp\", encoding='euc-kr')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.8 Create shp file based on raw service link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import geopandas\n",
    "from shapely.ops import linemerge\n",
    "from shapely.ops import unary_union\n",
    "\n",
    "seoul_link_raw = geopandas.read_file('seoul_link_raw.shp', encoding='utf-8')\n",
    "seoul_link_raw.sort_values(by=['SERVICE_ID', 'STD_ID'], inplace=True)\n",
    "seoul_link_raw.reset_index(drop=True, inplace=True)\n",
    "service_link_list = seoul_link_raw['SERVICE_ID'].unique()\n",
    "service_link_list.sort()\n",
    "\n",
    "seoul_link_raw['LINK_ID'] = seoul_link_raw['LINK_ID'].astype('int64')\n",
    "seoul_link_raw['F_NODE'] = seoul_link_raw['F_NODE'].astype('int64')\n",
    "seoul_link_raw['T_NODE'] = seoul_link_raw['T_NODE'].astype('int64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "seoul_service_link_raw = geopandas.GeoDataFrame(columns=[\n",
    "  'SERVICE_ID', 'F_NODE', 'T_NODE', 'NAME','LENGTH',\n",
    "  'AVG_LANES', 'MIN_LANES', 'MAX_LANES', 'LANE_CHANGE', 'AREA', 'geometry'])\n",
    "\n",
    "for i, idx in enumerate(service_link_list):\n",
    "  tmp_seoul_link_raw = seoul_link_raw[seoul_link_raw['SERVICE_ID'] == idx]\n",
    "  tmp_seoul_link_raw.reset_index(drop=True, inplace=True)\n",
    "\n",
    "  # Min/max/mean lanes and length to use it as a feature\n",
    "  min_lanes, max_lanes = tmp_seoul_link_raw['LANES'].min(), tmp_seoul_link_raw['LANES'].max()\n",
    "  link_length = round(tmp_seoul_link_raw['LENGTH'].sum(), 4)\n",
    "  area = (tmp_seoul_link_raw['LANES']*tmp_seoul_link_raw['LENGTH']).sum()\n",
    "  avg_lanes = round(area/link_length, 4)\n",
    "\n",
    "  # Get ininitial node(F NODE) and terminal node(T NODE)\n",
    "  f_node = tmp_seoul_link_raw['F_NODE'].unique()\n",
    "  t_node = tmp_seoul_link_raw['T_NODE'].unique()\n",
    "\n",
    "  f_node.sort()\n",
    "  t_node.sort()\n",
    "  f_node = f_node.tolist()\n",
    "  t_node = t_node.tolist()\n",
    "  \n",
    "  merged_line = unary_union(tmp_seoul_link_raw['geometry'])\n",
    "  tmp_list = [idx, str(f_node)[1:-1], str(t_node)[1:-1], tmp_seoul_link_raw.iloc[0]['ROAD_NAME'],\n",
    "              link_length, avg_lanes, min_lanes, max_lanes, min_lanes!=max_lanes, area, merged_line]\n",
    "\n",
    "  seoul_service_link_raw.loc[i] = tmp_list\n",
    "\n",
    "crs = seoul_link_raw.crs\n",
    "seoul_service_link_raw.crs = crs\n",
    "\n",
    "seoul_service_link_raw.set_geometry(col='geometry', inplace=True, drop=True)\n",
    "seoul_service_link_raw = seoul_service_link_raw.apply(pd.to_numeric, errors='ignore')\n",
    "\n",
    "seoul_service_link_raw.to_file(\"seoul_service_link_raw.shp\", encoding='euc-kr')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Distance related part"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. 1. Get shortest path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The value should be True\n",
      "True\n",
      "True\n",
      "The value should be False\n",
      "False\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas\n",
    "\n",
    "# Open link and node shapefile\n",
    "seoul_link_raw = geopandas.read_file('seoul_link_raw.shp', encoding='utf-8')\n",
    "seoul_node_raw = geopandas.read_file('seoul_node_raw.shp', encoding='utf-8')\n",
    "\n",
    "# In seoul_node_raw dataframe, there is x, y coordinate of node in 'geometry' column\n",
    "# Merge x and y coordinate of F_NODE and T_NODE to seoul_link_raw dataframe. It also have a suffix F_NODE and T_NODE\n",
    "seoul_link_raw = seoul_link_raw.merge(seoul_node_raw[['NODE_ID', 'geometry']], left_on='F_NODE', right_on='NODE_ID', how='left', suffixes=('', '_F_NODE'))\n",
    "# Drop coulmn named \"NODE_ID\" from seoul_link_raw dataframe\n",
    "seoul_link_raw.drop(columns=['NODE_ID'], inplace=True)\n",
    "\n",
    "seoul_link_raw = seoul_link_raw.merge(seoul_node_raw[['NODE_ID', 'geometry']], left_on='T_NODE', right_on='NODE_ID', how='left', suffixes=('', '_T_NODE'))\n",
    "# Drop coulmn named \"NODE_ID\" from seoul_link_raw dataframe\n",
    "seoul_link_raw.drop(columns=['NODE_ID'], inplace=True)\n",
    "\n",
    "# Add a column that has mean of x and y coordinate of F_NODE and T_NODE\n",
    "seoul_link_raw['x'] = seoul_link_raw.apply(lambda x: (x['geometry_F_NODE'].x + x['geometry_T_NODE'].x)/2, axis=1)\n",
    "seoul_link_raw['y'] = seoul_link_raw.apply(lambda x: (x['geometry_F_NODE'].y + x['geometry_T_NODE'].y)/2, axis=1)\n",
    "\n",
    "seoul_service_link_list = seoul_link_raw['SERVICE_ID'].unique()\n",
    "seoul_service_link_list.sort()\n",
    "\n",
    "# Group by 'SERVICE_LI' and calculate the mean coordinates for each group\n",
    "mean_coords = seoul_link_raw.groupby('SERVICE_ID')[['x', 'y']].mean()\n",
    "mean_coords.sort_index(inplace=True)\n",
    "\n",
    "# Calculate the absolute differences in x and y coordinates\n",
    "abs_diff = np.abs(mean_coords.values[:, None] - mean_coords.values)\n",
    "\n",
    "# Compute the 1D and 2D distances\n",
    "distance_1d = np.sum(abs_diff, axis=2) / 1000\n",
    "distance_2d = np.sqrt(np.sum(abs_diff ** 2, axis=2)) / 1000\n",
    "\n",
    "# Round the distances and convert them to DataFrames with the appropriate indices and columns\n",
    "distance_1d_df = pd.DataFrame(distance_1d, index=mean_coords.index, columns=mean_coords.index).round(5)\n",
    "distance_2d_df = pd.DataFrame(distance_2d, index=mean_coords.index, columns=mean_coords.index).round(5)\n",
    "\n",
    "# Replace distance smaller than 0.1 to 0.1\n",
    "distance_1d_df = distance_1d_df.where(distance_1d_df >= 0.1, 0.1)\n",
    "distance_2d_df = distance_2d_df.where(distance_2d_df >= 0.1, 0.1)\n",
    "\n",
    "# Check if the matrix is symmetric and contains inf value\n",
    "print('The value should be True')\n",
    "print(distance_1d_df.equals(distance_1d_df.T))\n",
    "print(distance_2d_df.equals(distance_2d_df.T))\n",
    "\n",
    "print('The value should be False')\n",
    "print(np.isinf(distance_1d_df).any().any())\n",
    "print(np.isinf(distance_2d_df).any().any())\n",
    "\n",
    "distance_1d_df.to_csv('distance_1d.csv', encoding='utf-8')\n",
    "distance_2d_df.to_csv('distance_2d.csv', encoding='utf-8')\n",
    "\n",
    "# Get the reciprocal of the distance matrix\n",
    "distance_1d_df = 1 / distance_1d_df\n",
    "distance_2d_df = 1 / distance_2d_df\n",
    "\n",
    "distance_1d_df.to_csv('distance_1d_a.csv', encoding='utf-8')\n",
    "distance_2d_df.to_csv('distance_2d_a.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import heapq\n",
    "import time\n",
    "\n",
    "def dijkstra(adj_matrix, dist_matrix, source):\n",
    "    n = len(adj_matrix)\n",
    "    visited = [False] * n\n",
    "    distances = [float('inf')] * n\n",
    "    distances[source] = 0\n",
    "    pq = [(0, source)]\n",
    "\n",
    "    while pq:\n",
    "        (dist, current) = heapq.heappop(pq)\n",
    "        if visited[current]:\n",
    "            continue\n",
    "        visited[current] = True\n",
    "\n",
    "        for neighbor in range(n):\n",
    "            if adj_matrix[current][neighbor] and not visited[neighbor]:\n",
    "                new_dist = dist + dist_matrix[current][neighbor]\n",
    "                if new_dist < distances[neighbor]:\n",
    "                    distances[neighbor] = new_dist\n",
    "                    heapq.heappush(pq, (new_dist, neighbor))\n",
    "\n",
    "    return distances\n",
    "\n",
    "\n",
    "def all_pairs_shortest_path(adj_matrix, dist_matrix):\n",
    "    n = len(adj_matrix)\n",
    "    all_pairs_sp = np.zeros((n, n))\n",
    "\n",
    "    start_time = time.time()\n",
    "    \n",
    "    for source in range(n):\n",
    "        all_pairs_sp[source] = dijkstra(adj_matrix, dist_matrix, source)\n",
    "\n",
    "        # Calculate progress percentage\n",
    "        progress = (source + 1) / n * 100\n",
    "        \n",
    "        # Calculate elapsed time and estimate time left\n",
    "        elapsed_time = time.time() - start_time\n",
    "        time_left = (elapsed_time / (source + 1)) * (n - source - 1)\n",
    "        if n%10 == 0:\n",
    "            print(f\"Progress: {progress:.2f}% | Time elapsed: {elapsed_time:.2f}s | Estimated time left: {time_left:.2f}s\")\n",
    "\n",
    "    return all_pairs_sp"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. 2. Getting minimum distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forward_a1 = pd.read_csv('seoul_adjacency_matrix/forward/seoul_forward_a1.csv', encoding='utf-8', index_col=0)\n",
    "distance_1d_df = pd.read_csv('distance_1d.csv', encoding='utf-8', index_col=['SERVICE_ID'])\n",
    "\n",
    "all_pairs_sp_matrix = all_pairs_shortest_path(forward_a1.values, distance_1d_df.values)\n",
    "print(\"All pairs shortest path matrix:\")\n",
    "print(all_pairs_sp_matrix)\n",
    "\n",
    "all_pairs_sp_df = pd.DataFrame(all_pairs_sp_matrix, index=distance_1d_df.index, columns=distance_1d_df.columns).round(5)\n",
    "all_pairs_sp_df.to_csv('seoul_adjacency_matrix/forward_sp_1d.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "backward_a1 = pd.read_csv('seoul_adjacency_matrix/backward/seoul_backward_a1.csv', encoding='utf-8', index_col=0)\n",
    "distance_1d_df = pd.read_csv('distance_1d.csv', encoding='utf-8', index_col=['SERVICE_ID'])\n",
    "\n",
    "all_pairs_sp_matrix = all_pairs_shortest_path(backward_a1.values, distance_1d_df.values)\n",
    "print(\"All pairs shortest path matrix:\")\n",
    "print(all_pairs_sp_matrix)\n",
    "\n",
    "all_pairs_sp_df = pd.DataFrame(all_pairs_sp_matrix, index=distance_1d_df.index, columns=distance_1d_df.columns).round(5)\n",
    "all_pairs_sp_df.to_csv('seoul_adjacency_matrix/backward_sp_1d.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_a1 = pd.read_csv('seoul_adjacency_matrix/total/seoul_total_a1.csv', encoding='utf-8', index_col=0)\n",
    "distance_1d_df = pd.read_csv('distance_1d.csv', encoding='utf-8', index_col=['SERVICE_ID'])\n",
    "\n",
    "all_pairs_sp_matrix = all_pairs_shortest_path(total_a1.values, distance_1d_df.values)\n",
    "print(\"All pairs shortest path matrix:\")\n",
    "print(all_pairs_sp_matrix)\n",
    "\n",
    "all_pairs_sp_df = pd.DataFrame(all_pairs_sp_matrix, index=distance_1d_df.index, columns=distance_1d_df.columns).round(5)\n",
    "all_pairs_sp_df.to_csv('seoul_adjacency_matrix/total_sp_1d.csv', encoding='utf-8')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. 3. Get minimum hop matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def floyd_warshall_min_hops(adj_matrix):\n",
    "    n = len(adj_matrix)\n",
    "    hop_matrix = adj_matrix.copy()\n",
    "\n",
    "    for k in range(n):\n",
    "        hop_matrix = np.minimum(hop_matrix, hop_matrix[:, k, np.newaxis] + hop_matrix[np.newaxis, k, :])\n",
    "\n",
    "    return hop_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.  3.  7. ... 13. 13. 12.]\n",
      " [ 5.  0.  4. ... 18. 18. 17.]\n",
      " [ 8. 10.  0. ... 21. 21. 20.]\n",
      " ...\n",
      " [16. 11. 15. ...  0.  1.  3.]\n",
      " [17. 13. 17. ...  3.  0.  2.]\n",
      " [16. 12. 16. ...  1.  2.  0.]]\n"
     ]
    }
   ],
   "source": [
    "forward_a1 = pd.read_csv('seoul_adjacency_matrix/forward/seoul_forward_a1.csv', encoding='utf-8', index_col=0)\n",
    "\n",
    "adj_matrix = forward_a1.values\n",
    "\n",
    "# Replace 0s with a large number (e.g., 1e9) except for the diagonal\n",
    "large_number = 1e9\n",
    "hop_matrix_modified = np.where(adj_matrix == 1, 1, large_number)\n",
    "np.fill_diagonal(hop_matrix_modified, 0)\n",
    "\n",
    "min_hop_matrix = floyd_warshall_min_hops(hop_matrix_modified)\n",
    "\n",
    "print(min_hop_matrix)\n",
    "\n",
    "min_hop_matrix = pd.DataFrame(min_hop_matrix, index=forward_a1.index, columns=forward_a1.columns)\n",
    "min_hop_matrix.to_csv('seoul_adjacency_matrix/forward_min_hop.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.  5.  8. ... 16. 17. 16.]\n",
      " [ 3.  0. 10. ... 11. 13. 12.]\n",
      " [ 7.  4.  0. ... 15. 17. 16.]\n",
      " ...\n",
      " [13. 18. 21. ...  0.  3.  1.]\n",
      " [13. 18. 21. ...  1.  0.  2.]\n",
      " [12. 17. 20. ...  3.  2.  0.]]\n"
     ]
    }
   ],
   "source": [
    "backward_a1 = pd.read_csv('seoul_adjacency_matrix/backward/seoul_backward_a1.csv', encoding='utf-8', index_col=0)\n",
    "\n",
    "adj_matrix = backward_a1.values\n",
    "\n",
    "# Replace 0s with a large number (e.g., 1e9) except for the diagonal\n",
    "large_number = 1e9\n",
    "hop_matrix_modified = np.where(adj_matrix == 1, 1, large_number)\n",
    "np.fill_diagonal(hop_matrix_modified, 0)\n",
    "\n",
    "min_hop_matrix = floyd_warshall_min_hops(hop_matrix_modified)\n",
    "\n",
    "print(min_hop_matrix)\n",
    "\n",
    "min_hop_matrix = pd.DataFrame(min_hop_matrix, index=backward_a1.index, columns=backward_a1.columns)\n",
    "min_hop_matrix.to_csv('seoul_adjacency_matrix/backward_min_hop.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.  3.  5. ... 11. 12. 11.]\n",
      " [ 3.  0.  4. ... 11. 12. 11.]\n",
      " [ 5.  4.  0. ... 15. 16. 15.]\n",
      " ...\n",
      " [11. 11. 15. ...  0.  1.  1.]\n",
      " [12. 12. 16. ...  1.  0.  2.]\n",
      " [11. 11. 15. ...  1.  2.  0.]]\n"
     ]
    }
   ],
   "source": [
    "total_a1 = pd.read_csv('seoul_adjacency_matrix/total/seoul_total_a1.csv', encoding='utf-8', index_col=0)\n",
    "\n",
    "adj_matrix = total_a1.values\n",
    "\n",
    "# Replace 0s with a large number (e.g., 1e9) except for the diagonal\n",
    "large_number = 1e9\n",
    "hop_matrix_modified = np.where(adj_matrix == 1, 1, large_number)\n",
    "np.fill_diagonal(hop_matrix_modified, 0)\n",
    "\n",
    "min_hop_matrix = floyd_warshall_min_hops(hop_matrix_modified)\n",
    "\n",
    "print(min_hop_matrix)\n",
    "\n",
    "min_hop_matrix = pd.DataFrame(min_hop_matrix, index=total_a1.index, columns=total_a1.columns)\n",
    "min_hop_matrix.to_csv('seoul_adjacency_matrix/total_min_hop.csv', encoding='utf-8')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('re1')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b338a3c89f97a4b5827ba51798bf3d2d19846871941c7ddf439540a5fe1b4084"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
